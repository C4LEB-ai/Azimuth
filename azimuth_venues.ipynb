{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2306ab9b",
   "metadata": {},
   "source": [
    "# azimuth venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f26dfa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import supabase\n",
    "import pandas as pd\n",
    "from postgrest import APIError\n",
    "from supabase import create_client, Client\n",
    "\n",
    "supabase_url: str = os.getenv(\"SUPABASE_URL\")\n",
    "supabase_key: str = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase_client: Client = create_client(SUPABASE_URL, SUPABASE_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e2fba0",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------\n",
    "    venues from seatgeek database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "a4de4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = supabase_client.table('seatgeek_data').select(\"*\").execute()\n",
    "def seatGeek(response):\n",
    "    # Create a DataFrame from the API response data\n",
    "    df = pd.DataFrame(response.data)\n",
    "\n",
    "    # Reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Remove duplicates based on 'venue.name' column\n",
    "    venue = df[~df['venue.name'].duplicated()]\n",
    "\n",
    "    # Get the columns of interest\n",
    "    cols = list(venue.columns)\n",
    "    data = venue[cols[27:-12]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "760f0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seatGeek(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a9d5dc",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "    _venues from ticketmaster database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "dcd533b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = supabase_client.table('ticketmasterdata').select(\"*\").execute()\n",
    "def ticketMaster(response):\n",
    "    # Create a DataFrame from the API response data\n",
    "    tkMaster_df = pd.DataFrame(response.data)\n",
    "\n",
    "    # Reset the index\n",
    "    tkMaster_df = tkMaster_df.reset_index(drop=True)\n",
    "\n",
    "    # Select the columns of interest\n",
    "    tkMaster_df = tkMaster_df.iloc[:, 17:-7]\n",
    "\n",
    "    # Remove duplicates based on 'Venue Name' column\n",
    "    tkMaster_df = tkMaster_df[~tkMaster_df['Venue Name'].duplicated()]\n",
    "\n",
    "    return tkMaster_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "096faa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ticketMaster(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c01c94",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------\n",
    "    Venues.bson file from crankDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2efd05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processVenuesData(filename):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    venues = pd.read_csv(filename)\n",
    "\n",
    "    # Remove duplicates based on 'Name' column\n",
    "    venues = venues[~venues['Name'].duplicated()]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    venues.drop(['MetroArea._id', '_id'], axis=1, inplace=True)\n",
    "\n",
    "    return venues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "5b20fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processVenuesData('venues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e36617",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "    chceking if there exist similarity between the unique values in tkmaster and crankdb\n",
    "    \n",
    "--------------------------------------------\n",
    "\n",
    "    checking if the values in ticketmaster db exists in seatgeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "5d05624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10454/630730371.py:3: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  venues = pd.read_csv(filename)\n",
      "/tmp/ipykernel_10454/630730371.py:3: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  venues = pd.read_csv(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126749, 20)\n",
      "(33, 6)\n"
     ]
    }
   ],
   "source": [
    "common_vanues = processVenuesData('venues.csv')['Name'].isin(seatGeek(response)['venue.name'])\n",
    "crankdb_nique = processVenuesData('venues.csv')[~common_vanues]\n",
    "print(crankdb_nique.shape)\n",
    "\n",
    "comon = ticketMaster(response2)['Venue Name'].isin(crankdb_unique['name'])\n",
    "tkMaster_unique = ticketMaster(response2)[comon]\n",
    "print(tkMaster_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8071305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((126749, 14), (3054, 30), (33, 6))"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crankdb_unique.shape, seatGeek(response).shape, tkMaster_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "33c5186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seatGeek = seatGeek(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "b29ea5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def processSeatGeekData(seatGeek):\n",
    "    # Rename columns\n",
    "    cols = list(seatGeek.columns)\n",
    "    seatGeek.columns = [i.replace('venue.', '').lower() for i in cols]\n",
    "\n",
    "    # Select desired columns\n",
    "    seatGeek = seatGeek[['name', 'state', 'country', 'postal_code', 'city', 'capacity', 'popularity', 'address', 'url', 'timezone', 'location.lat', 'location.lon']]\n",
    "\n",
    "    # Rename columns\n",
    "    seatGeek.columns = ['name', 'state', 'country', 'postalCode', 'city', 'capacity', 'popularity', 'address', 'url', 'timezone', 'longitude', 'latitude']\n",
    "\n",
    "    # Add new columns\n",
    "    seatGeek['description'] = None\n",
    "    seatGeek['phone'] = ''\n",
    "    seatGeek['metroareaName'] = None\n",
    "\n",
    "    return seatGeek\n",
    "\n",
    "seatGeek = processSeatGeekData(seatGeek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "4c135908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'state', 'country', 'city', 'capacity', 'address',\n",
       "       'metroarea_name', 'phone', 'url', 'description', 'longitude',\n",
       "       'latitude', 'postal_code', 'timezone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crankdb_unique['postal_code'] = None\n",
    "crankdb_unique['timezone'] = None\n",
    "crankdb_unique.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54f105",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "    joining the datasets\n",
    " ---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a7b79e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "azimuthVenues = pd.concat((seatGeek, crankdb_unique), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "194e9765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129803, 17)"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "azimuthVenues.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968a707",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "\n",
    "    inserting the following data into our database table\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadToDb(azimuthVenues, supabase_client):\n",
    "    # Replace NaN values with None\n",
    "    azimuthVenues.replace({pd.np.nan: None}, inplace=True)\n",
    "\n",
    "    # Convert DataFrame to JSON records\n",
    "    json_data = azimuthVenues.to_dict(orient='records')\n",
    "\n",
    "    # Insert records into the database\n",
    "    for record in json_data:\n",
    "        try:\n",
    "            supabase_client.table('azimuth_venues').insert(record).execute()\n",
    "        except APIError as e:\n",
    "            print(\"Error occurred while inserting the record:\")\n",
    "            print(record)\n",
    "            print(\"Error message:\", e.message)\n",
    "            print()\n",
    "\n",
    "loadToDb(azimuthVenues, supabase_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b75223",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "\n",
    "    removing duplicated rows\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293c7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDups(supabase_client):\n",
    "    response = supabase_client.table('azimuth_venues').select('*').execute()\n",
    "    data = response.data\n",
    "\n",
    "    # Create a set to store unique IDs\n",
    "    unique_ids = set()\n",
    "\n",
    "    # List to store rows to be deleted\n",
    "    rows_to_delete = []\n",
    "\n",
    "    # Iterate through each row and filter duplicates based on 'name'\n",
    "    for row in data:\n",
    "        row_name = row['name']\n",
    "        if row_name not in unique_ids:\n",
    "            unique_ids.add(row_name)\n",
    "        else:\n",
    "            rows_to_delete.append(row_name)\n",
    "\n",
    "    # Delete the duplicate rows from the table\n",
    "    for row_name in rows_to_delete:\n",
    "        # Retrieve the first occurrence of the duplicate row\n",
    "        first_occurrence = next(row for row in data if row['name'] == row_name)\n",
    "        # Delete all other occurrences of the duplicate row\n",
    "        supabase_client.table('azimuth_venues').delete().match({'name': row_name}).execute()\n",
    "        # Insert the first occurrence back into the table\n",
    "        supabase_client.table('azimuth_venues').insert(first_occurrence).execute()\n",
    "\n",
    "    print(\"Duplicate rows retained and others deleted successfully.\")\n",
    "\n",
    "filterDups(supabase_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
